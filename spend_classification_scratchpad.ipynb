{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = os.path.normpath('C:/Users/daveysa1/Documents/GitHub/Random')\n",
    "file = 'Data.csv'\n",
    "filepath = os.path.join(folder,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pandas.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3689, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into Train and Test sets\n",
    "df_train = df.ix[:round(len(df)*0.75), 'Description':'Category'] # 3/4 of the rows, description and category columns \n",
    "df_test = df.ix[round(len(df)*0.75)+1:, 'Description':'Category'] # 1/4 of the rows, description and category columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 3689\n",
      "train: 2768\n",
      "test: 921\n",
      "combined: 3689\n",
      ">>> OK!\n"
     ]
    }
   ],
   "source": [
    "# check that our indexing worked\n",
    "print('total:', len(df))\n",
    "print('train:', len(df_train))\n",
    "print('test:', len(df_test))\n",
    "print('combined:', len(df_train) + len(df_test))\n",
    "if len(df) == len(df_train) + len(df_test):\n",
    "    print('>>> OK!')\n",
    "else:\n",
    "    print('>>> ERROR!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all text has to be tokenised before it can be useful.\n",
    "http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html suggests using Bag of Words to tokenize and look at frequency in order to determine classification. Transaction descriptions have few and relatively unique words, so Bag of Words should give precise classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "look up principal component analysis / feature reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# instantiate the vectorizer\n",
    "vectoriser = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vectorizers are stateless, so fit() does nothing; you need to fit_transform.\n",
    "# fit_transform() turns the sequence (transaction descriptions) into a matrix of words\n",
    "\n",
    "# transform the whole set of Descriptions. The whole set because we are not training at this point, just wrangling.\n",
    "# Words in each description are tokenised during the transform.\n",
    "df_train_xform = vectoriser.fit_transform(df_train.ix[:,'Description'])\n",
    "df_test_xform = vectoriser.transform(df_test.ix[:,'Description'])\n",
    "new_descriptions = ['MYER MORLEY','COLES Bassendean','JB HIFI','Caltex Sydney','Liquorland Kalgoorlie']\n",
    "manual_test = vectoriser.transform(new_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top of training set:\n",
      "                               Description           Category\n",
      "1                      DEBIT INT TO 29 JUN  Mortgage Interest\n",
      "2  HOUSE IN MORLEY          MORLEY      WA         Renovation\n",
      "\n",
      "transformed training set array:\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "\n",
      "transformed TRAINing set shape:\n",
      "(2768, 1957)\n",
      "\n",
      "transformed TESTing set shape:\n",
      "(921, 1957)\n",
      "\n",
      "transformed MANUAL PREDICT set shape:\n",
      "(5, 1957)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('top of training set:')\n",
    "print(df_train.ix[1:2,:])\n",
    "print()\n",
    "print('transformed training set array:')\n",
    "print(df_train_xform[:2].toarray())\n",
    "print()\n",
    "\n",
    "print('transformed TRAINing set shape:')\n",
    "print(df_train_xform.shape)\n",
    "print()\n",
    "print('transformed TESTing set shape:')\n",
    "print(df_test_xform.shape)\n",
    "print()\n",
    "print('transformed MANUAL PREDICT set shape:')\n",
    "print(manual_test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# the classifier is not instantiated - all static\n",
    "# it is fit() to the transformed training set, linked to the relevant Categories\n",
    "clf = MultinomialNB().fit(df_train_xform, df_train.ix[:, 'Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'MYER MORLEY' => Renovation\n",
      "'COLES Bassendean' => Living and Bills\n",
      "'JB HIFI' => Renovation\n",
      "'Caltex Sydney' => Living and Bills\n",
      "'Liquorland Kalgoorlie' => Living and Bills\n"
     ]
    }
   ],
   "source": [
    "predicted = clf.predict(X_new_counts)\n",
    "for description, category in zip(new_descriptions, predicted):\n",
    "    print('%r => %s' % (description, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hashing version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1983"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of distinct words in the transaction descriptions\n",
    "# This is needed for the n_features parameter of the HashingVectorizer\n",
    "from collections import Counter\n",
    "c = Counter()\n",
    "for desc in df_train.ix[:, 'Description']:\n",
    "    c.update(desc.lower().split())\n",
    "\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # instantiate the vectorizer\n",
    "# vectoriser = HashingVectorizer(#input='content', # default: we're passing data directly rather than asking to analyze 1..n files\n",
    "#                                #encoding='utf-8', # default\n",
    "#                                #decode_error='strict', # default: raise UnicodeDecodeError if a sketchy char is found\n",
    "#                                #strip_accents=None, # defaul: we expect transaction descriptions to be ascii anyway\n",
    "#                                #lowercase=True, # default\n",
    "#                                preprocessor=None,  # default\n",
    "#                                tokenizer=None,  # default\n",
    "#                                stop_words=None, # merchant names will have useful 'stop-words' that we want to keep\n",
    "#                                #token_pattern='(?u)\\b\\w\\w+\\b', # default: keep the regexp\n",
    "#                                ngram_range=(1, 1), \n",
    "#                                analyzer='word', # hash the words, not the individual characters\n",
    "#                                n_features=10000, # we've counted the dustinct words and keeping all as features\n",
    "#                                binary=False, \n",
    "#                                norm='l2', \n",
    "#                                non_negative=False, \n",
    "#                                #dtype=<class 'numpy.float64'>\n",
    "#                               )\n",
    "# # HashingVectorizers are stateless, so fit() does nothing; you need to fit_transform.\n",
    "# # fit_transform() turns the sequence (transaction descriptions) into a matrix of words\n",
    "\n",
    "# # transform the whole set of Descriptions. The whole set because we are not training at this point, just wrangling.\n",
    "# df_xform = vectoriser.fit_transform(df.ix[:,'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn import tree\n",
    "training_set = df_xform.ix[:round(len(df_xform)*0.75), 'Description':'Category'] # 3/4 of the rows, description and category columns \n",
    "testing_set = df_xform.ix[round(len(df_xform)*0.75)+1:, 'Description'] # The other 1/4 of the rows, description column only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Front Bathroom'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-b3ce1ffad0a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\daveysa1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\daveysa1\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    371\u001b[0m                                       force_all_finite)\n\u001b[0;32m    372\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 373\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Front Bathroom'"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(training_set, testing_set)\n",
    "# throws exception because it is expecting floats rather than strings. 'Front Bathroom' is used at the bottom boundary of training_set and the top boundary of testing_set.\n",
    "# pd.getdummies() will convert a list of features into a separate 1/0 vector for each, but this won't solve the problem of using string descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
